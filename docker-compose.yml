services:
  data_parser:
    image: aixii_data_parser:latest
    build:
      context: src
      dockerfile: Dockerfile
    container_name: ./data_parser
    depends_on:
      - llama3
    volumes:
      - /api_data:/app/api_data
    environment:
      - PYTHONUNBUFFERED=1
    env_file:
      - ./PDF_LLM_Parser/.env
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  llama3:
    image: asg_llama3:latest
    build:
      context: LLM_Server
      dockerfile: LLM_Server/Dockerfile
    container_name: llama3-api
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    networks:
      - ollama-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-ui
    depends_on:
      - llama3
    ports:
      - "8080:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - OLLAMA_API_BASE_URL=http://host.docker.internal:11434
      - WEBUI_SECRET_KEY=ai12
      - CORS_ALLOW_ORIGIN=*
      - DISABLE_SIGNUP=false
      - RAG_ENABLED=true
      - EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - AUTH_REQUIRED=true
      - ADMIN_EMAIL=ogienko.12003@gmail.com
    volumes:
      - ./open-webui:/app/backend/data
      - ./rag_storage:/app/rag_storage
    restart: unless-stopped
    networks:
      - ollama-net
      - my-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

networks:
  ollama-net:
    name: ollama-net
    driver: bridge
    attachable: true
  my-net:
    external: true